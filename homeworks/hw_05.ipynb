{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 5\n",
    "\n",
    "**Due: 03/09/2018** (Friday 9th March at 11:59pm).\n",
    "\n",
    "## Instructions\n",
    "\n",
    "+ In any case, develop the code and generate the figures you need to solve the problems using this notebook.\n",
    "+ For the answers that require a mathematical proof or derivation you can either:\n",
    "    \n",
    "    - Type the answer using the built-in latex capabilities. In this case, simply export the notebook as a pdf and upload it on gradescope; or\n",
    "    - you can print the notebook (after you are done with all the code), write your answers by hand, scan, turn your response to a single pdf, and upload on gradescope. \n",
    "\n",
    "+ The total homework points are 100. Please note that the problems are not weighed equally.\n",
    "\n",
    "**Note**: Please match all the pages corresponding to each of the questions when you submit on gradescope. \n",
    "\n",
    "## Student details\n",
    "\n",
    "+ **First Name:**\n",
    "+ **Last Name:**\n",
    "+ **Email:**\n",
    "\n",
    "## Readings\n",
    "\n",
    "Before attempting the homework, it is probably a good idea to:\n",
    "+ Review the slides of lectures 13 and 14; and\n",
    "+ Review the corresponding lecture handouts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\newcommand{\\x}{\\mathbf{x}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "import fipy\n",
    "import GPy\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "\n",
    "class KarhunenLoeveExpansion(object):\n",
    "    \n",
    "    \"\"\"\n",
    "    A class representing the Karhunen Loeve Expansion of a Gaussian random field.\n",
    "    It uses the Nystrom approximation to do it.\n",
    "    \n",
    "    Arguments:\n",
    "        k      -     The covariance function.\n",
    "        Xq     -     Quadrature points for the Nystrom approximation.\n",
    "        wq     -     Quadrature weights for the Nystrom approximation.\n",
    "        alpha  -     The percentage of the energy of the field that you want to keep.\n",
    "        X      -     Observed inputs (optional).\n",
    "        Y      -     Observed field values (optional).\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, k, Xq=None, wq=None, nq=100, alpha=0.9, X=None, Y=None):\n",
    "        self.k = k\n",
    "        if Xq is None:\n",
    "            if k.input_dim == 1:\n",
    "                Xq = np.linspace(0, 1, nq)[:, None]\n",
    "                wq = np.ones((nq, )) / nq\n",
    "            elif k.input_dim == 2:\n",
    "                nq = int(np.sqrt(nq))\n",
    "                x = np.linspace(0, 1, nq)\n",
    "                X1, X2 = np.meshgrid(x, x)\n",
    "                Xq = np.hstack([X1.flatten()[:, None], X2.flatten()[:, None]])\n",
    "                wq = np.ones((nq ** 2, )) / nq ** 2\n",
    "            else:\n",
    "                raise NotImplementedError('For more than 2D, please supply quadrature points and weights.')\n",
    "        self.Xq = Xq\n",
    "        self.wq = wq\n",
    "        self.k = k\n",
    "        self.alpha = alpha\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        # If we have some observed data, we need to use the posterior covariance\n",
    "        if X is not None:\n",
    "            gpr = GPy.models.GPRegression(X, Y[:, None], k)\n",
    "            gpr.likelihood.variance = 1e-12\n",
    "            self.gpr = gpr\n",
    "            Kq = gpr.predict(Xq, full_cov=True)[1]\n",
    "        else:\n",
    "            Kq = k.K(Xq)\n",
    "        B = np.einsum('ij,j->ij', Kq, wq)\n",
    "        lam, v = scipy.linalg.eigh(B, overwrite_a=True)\n",
    "        lam = lam[::-1]\n",
    "        lam[lam <= 0.] = 0.\n",
    "        energy = np.cumsum(lam) / np.sum(lam)\n",
    "        i_end = np.arange(energy.shape[0])[energy > alpha][0] + 1\n",
    "        lam = lam[:i_end]\n",
    "        v = v[:, ::-1]\n",
    "        v = v[:, :i_end]\n",
    "        self.lam = lam\n",
    "        self.sqrt_lam = np.sqrt(lam)\n",
    "        self.v = v\n",
    "        self.energy = energy\n",
    "        self.num_xi = i_end\n",
    "        \n",
    "    def eval_phi(self, x):\n",
    "        \"\"\"\n",
    "        Evaluate the eigenfunctions at x.\n",
    "        \"\"\"\n",
    "        if self.X is not None:\n",
    "            nq = self.Xq.shape[0]\n",
    "            Xf = np.vstack([self.Xq, x])\n",
    "            m, C = self.gpr.predict(Xf, full_cov=True)\n",
    "            Kc = C[:nq, nq:].T\n",
    "            self.tmp_mu = m[nq:, :].flatten()\n",
    "        else:\n",
    "            Kc = self.k.K(x, self.Xq)\n",
    "            self.tmp_mu = 0.\n",
    "        phi = np.einsum(\"i,ji,j,rj->ri\", 1. / self.lam, self.v, self.wq**0.5, Kc)\n",
    "        return phi\n",
    "    \n",
    "    def __call__(self, x, xi):\n",
    "        \"\"\"\n",
    "        Evaluate the expansion at x and xi.\n",
    "        \"\"\"\n",
    "        phi = self.eval_phi(x)\n",
    "        return self.tmp_mu + np.dot(phi, xi * self.sqrt_lam)\n",
    "\n",
    "class Elliptic2DSolver(object):\n",
    "    def __init__(self, nx=100, ny=100, value_left=1.,\n",
    "                 value_right=0., value_top=0., value_bottom=0.):\n",
    "        \"\"\"\n",
    "        ::param nx:: Number of cells in the x direction.\n",
    "        ::param ny:: Number of cells in the y direction.\n",
    "        ::param value_left:: Boundary condition on the left face.\n",
    "        ::param value_right:: Boundary condition on the right face.\n",
    "        ::param value_top:: Boundary condition on the top face.\n",
    "        ::param value_bottom:: Boundary condition on the bottom face.\n",
    "        \"\"\"\n",
    "        #set domain dimensions\n",
    "        self.nx = nx\n",
    "        self.ny = ny\n",
    "        self.dx = 1. / nx\n",
    "        self.dy = 1. / ny\n",
    "        \n",
    "        #define mesh\n",
    "        self.mesh = fipy.Grid2D(nx=self.nx, ny=self.ny, dx=self.dx, dy=self.dy)\n",
    "        \n",
    "        #get all the face centers \n",
    "        X, Y = self.mesh.faceCenters.value\n",
    "        X = X[:, None]\n",
    "        Y = Y[:, None]\n",
    "        self.Xface = np.hstack([X,Y])\n",
    "\n",
    "        #define cell and face variables \n",
    "        self.phi = fipy.CellVariable(name='$T(x)$', mesh=self.mesh, value=1.)\n",
    "        self.C = fipy.FaceVariable(name='$C(x)$', mesh=self.mesh, value=1.)\n",
    "        self.source=fipy.CellVariable(name='$f(x)$', mesh=self.mesh, value=0.)\n",
    "        \n",
    "        #apply boundary conditions\n",
    "        #dirichet\n",
    "        self.phi.constrain(value_left, self.mesh.facesLeft)\n",
    "        self.phi.constrain(value_right, self.mesh.facesRight)\n",
    "        \n",
    "        #homogeneous Neumann\n",
    "        self.phi.faceGrad.constrain(value_top, self.mesh.facesTop)\n",
    "        self.phi.faceGrad.constrain(value_bottom, self.mesh.facesBottom)\n",
    "        \n",
    "        #setup the diffusion problem\n",
    "        self.eq = -fipy.DiffusionTerm(coeff=self.C) == self.source\n",
    "        \n",
    "    def set_coeff(self, C):\n",
    "        \"\"\"\n",
    "        Initialize the random conductivity field.\n",
    "        \"\"\"\n",
    "        self.C.setValue(C)\n",
    "    \n",
    "    def solve(self):\n",
    "        self.eq.solve(var=self.phi)\n",
    "        #return self.phi.value\n",
    "    \n",
    "    def __call__(self):\n",
    "        self.solve()\n",
    "        return self.phi.value.flatten()\n",
    "\n",
    "def sample(solver, kle, nsamples=1):\n",
    "    \"\"\"\n",
    "    solver -> An object of the Elliptic2DSolver object. \n",
    "    kle -> An object of the KarhunenLoeveExpansion class.\n",
    "    nsamples -> Number of samples of the PDE solution to generate.\n",
    "    \"\"\"\n",
    "    samples = np.zeros((nsamples, solver.nx*solver.ny))\n",
    "    numxi = kle.num_xi\n",
    "    for i in xrange(nsamples):\n",
    "        xi = np.random.randn(numxi)\n",
    "        coeff = kle(solver.Xface, xi)\n",
    "        solver.set_coeff(np.exp(coeff))\n",
    "        sample = solver()\n",
    "        samples[i] = sample\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1 - Uncertainty propagation in 2-D stochastic elliptic partial differential equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In lecture 13, we discussed the Monte Carlo method as a technique to compute high-dimensional integrals and used this technique to propagate uncertainties through physical models. \n",
    "\n",
    "Consider the following partial differential equation:\n",
    "\n",
    "$$\n",
    "\\nabla (a(\\x) \\nabla u(\\x)) = 0, \\ \\forall \\x \\in [0, 1]^2,\n",
    "$$\n",
    "with boundary conditions:\n",
    "$$\n",
    "u|_{x_1 = 0} = 1, \\\\\n",
    "u|_{x_1 = 1} = 0, \\\\\n",
    "\\frac{\\partial u}{\\partial n}|_{x_2 = 0} = 0, \\\\\n",
    "\\frac{\\partial u}{\\partial n}|_{x_2 = 1} = 0.\n",
    "$$\n",
    "\n",
    "This is a general model for diffusion problems and shows up in a wide variety of applications, such as heat conduction, subsurface flows, electromagnetics etc. It is often the case that one is uncertain about the diffusion coefficient $a$. Suppose it is known that $a$ is a positive quantity and it varies with lengthscales 0.1 and 0.5 along the two spatial directions. Assume that $a$ is a smooth quantity and has unit variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part a."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pose the diffusion coefficient as a transformed Gaussian random field. Use the Karhunen-Loeve expansion to obtain a reduced representation of $a$. Make sure you use enough quadrature points for the KLE.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part b."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class ```Elliptic2DSolver``` defined at the top of this notebook can be used to solve this PDE. Additionally, we have provided a function ```sample``` to generate as many samples of the stochastic PDE solution as you want. \n",
    "\n",
    "Establish the ground truth on the mean and variance of the solution $u$ of the PDE using 100000 Monte Carlo samples of the uncertain diffusion field. Show contour plots for mean and 2 standard deviation fields of the solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part c."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Propagate uncertainty through the model using 10, 100, 1,000, and 10,000 MC samples and plot the *relative* L2 error in the estimation of the mean and the variance. The relative L2 error is defined as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Relative L2 Error\n",
    "Let $\\mathbf{y}_{\\mbox{true}}$ be the *ground truth vector* and $\\hat{\\mathbf{y}}$ be the *estimated vector*.\n",
    "The relattive L2 error is defined to be:\n",
    "$$\n",
    "L_2[\\mathbf{y}_{\\mbox{true}},\\hat{\\mathbf{y}}] = \\frac{\\parallel\\mathbf{y}_{\\mbox{true}}-\\hat{\\mathbf{y}} \\parallel_2}{\\parallel \\mathbf{y}_{\\mbox{true}}\\parallel_2},\n",
    "$$\n",
    "where $\\parallel\\cdot\\parallel_2$ is the standard Euclidean norm.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part d."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat c. 100 times (if it takes too much time do 10) to get the uncertainty induced by the fact that MC estimates are noisy. You can use the function [numpy.percentile](http://docs.scipy.org/doc/numpy-dev/reference/generated/numpy.percentile.html) to get lower and upper uncertainty bars for the evolution of the L2 error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2  - Latin hypercube sampling (LHS)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat the tasks in problem 1 parts (b), (c) and (d) with Latin hypercube sampling (LHS). Compare the LHS results with MCS results.\n",
    "\n",
    "#### Hint:\n",
    "Recall that we has used the Latin Hypercube sampler in class to generate uniform samples in the unit square - $[0, 1]^2$. It is trivial to transform these uniform samples into standard normal distributed samples. We just need the inverse CDF method discussed in Lecture 5. Also recall, from homework 2, that ```scipy.stat.norm.ppf``` implements the inverse CDF of the standard normal random variable. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
