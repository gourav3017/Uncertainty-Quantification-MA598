{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 7\n",
    "\n",
    "**Due: 04/24/2018** (Tuesday 24th April at 11:59pm).\n",
    "\n",
    "## Instructions\n",
    "\n",
    "+ In any case, develop the code and generate the figures you need to solve the problems using this notebook.\n",
    "+ For the answers that require a mathematical proof or derivation you can either:\n",
    "    \n",
    "    - Type the answer using the built-in latex capabilities. In this case, simply export the notebook as a pdf and upload it on gradescope; or\n",
    "    - you can print the notebook (after you are done with all the code), write your answers by hand, scan, turn your response to a single pdf, and upload on gradescope. \n",
    "\n",
    "+ The total homework points are 100. Please note that the problems are not weighed equally.\n",
    "\n",
    "**Note**: Please match all the pages corresponding to each of the questions when you submit on gradescope. \n",
    "\n",
    "## Student details\n",
    "\n",
    "+ **First Name:**\n",
    "+ **Last Name:**\n",
    "+ **Email:**\n",
    "\n",
    "## Readings\n",
    "\n",
    "Before attempting the homework, it is probably a good idea to:\n",
    "+ Review the slides of lectures 20, 21, 22, 23 and 24 ; and\n",
    "+ Review the corresponding lecture handouts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pymc as pm\n",
    "import math\n",
    "import scipy.stats as st\n",
    "import scipy\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "mpl.rcParams['figure.dpi'] = 300\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "import design\n",
    "import orthpol\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from tqdm import tqdm # pip install tqdm (or conda)\n",
    "np.set_printoptions(suppress=True)\n",
    "from pymc.Matplot import plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Catalysis problem - Calibrating reaction rate coefficients with ```pyMC```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that we used the problem of calibrating reaction rate coefficients in a catalytic reaction as the running example for demonstrating various approaches to solving inverse problems - beginning with the classical approach where this task is posed as the minimization of a misfit function, to the probabilistic approach where the inverse problem is posed as a Bayesian inference task. In this assignment, we will re-visit the catalysis problem (yet again !), this time solving it with ```pyMC```. Working through this assignment should help you get comfortable with probabilistic programming. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first load the data. Recall that the experimental data is simply the measurement of 5 chemical substances at 6 different timesteps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NO3</th>\n",
       "      <th>NO2</th>\n",
       "      <th>N2</th>\n",
       "      <th>NH3</th>\n",
       "      <th>N2O</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>500.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>250.95</td>\n",
       "      <td>107.32</td>\n",
       "      <td>18.51</td>\n",
       "      <td>3.33</td>\n",
       "      <td>4.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>123.66</td>\n",
       "      <td>132.33</td>\n",
       "      <td>74.85</td>\n",
       "      <td>7.34</td>\n",
       "      <td>20.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>84.47</td>\n",
       "      <td>98.81</td>\n",
       "      <td>166.19</td>\n",
       "      <td>13.14</td>\n",
       "      <td>42.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>30.24</td>\n",
       "      <td>38.74</td>\n",
       "      <td>249.78</td>\n",
       "      <td>19.54</td>\n",
       "      <td>55.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>27.94</td>\n",
       "      <td>10.42</td>\n",
       "      <td>292.32</td>\n",
       "      <td>24.07</td>\n",
       "      <td>60.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>13.54</td>\n",
       "      <td>6.11</td>\n",
       "      <td>309.50</td>\n",
       "      <td>27.26</td>\n",
       "      <td>62.54</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         NO3     NO2      N2    NH3    N2O\n",
       "Time                                      \n",
       "0     500.00    0.00    0.00   0.00   0.00\n",
       "30    250.95  107.32   18.51   3.33   4.98\n",
       "60    123.66  132.33   74.85   7.34  20.14\n",
       "90     84.47   98.81  166.19  13.14  42.10\n",
       "120    30.24   38.74  249.78  19.54  55.98\n",
       "150    27.94   10.42  292.32  24.07  60.65\n",
       "180    13.54    6.11  309.50  27.26  62.54"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The data\n",
    "import pandas as pd\n",
    "catalysis_data = pd.read_csv('catalysis.csv', index_col=0)\n",
    "Y = catalysis_data[1:].get_values()\n",
    "catalysis_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[250.95 123.66  84.47  30.24  27.94  13.54 107.32 132.33  98.81  38.74\n",
      "  10.42   6.11  18.51  74.85 166.19 249.78 292.32 309.5    3.33   7.34\n",
      "  13.14  19.54  24.07  27.26   4.98  20.14  42.1   55.98  60.65  62.54]\n"
     ]
    }
   ],
   "source": [
    "print Y.flatten(order='F')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also require the model (i.e. solver) for the dynamical system governing the catalytic conversion. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For making predictions\n",
    "import scipy.integrate\n",
    "\n",
    "def A(x):\n",
    "    \"\"\"\n",
    "    Return the matrix of the dynamical system.\n",
    "    \"\"\"\n",
    "    # Scale back to the k's\n",
    "    k = np.exp(x) / 180.\n",
    "    res = np.zeros((6,6))\n",
    "    res[0, 0] = -k[0]\n",
    "    res[1, 0] = k[0]\n",
    "    res[1, 1] = -(k[1] + k[3] + k[4])\n",
    "    res[2, 1] = k[1]\n",
    "    res[2, 2] = -k[2]\n",
    "    res[3, 2] = k[2]\n",
    "    res[4, 1] = k[4]\n",
    "    res[5, 1] = k[3]\n",
    "    return res\n",
    "\n",
    "def g(z, t, x):\n",
    "    \"\"\"\n",
    "    The right hand side of the dynamical system.\n",
    "    \"\"\"\n",
    "    return np.dot(A(x), z)\n",
    "\n",
    "\n",
    "# The full solution of the dynamical system\n",
    "def Z(x, t):\n",
    "    \"\"\"\n",
    "    Returns the solution for parameters x at times t.\n",
    "    \"\"\"\n",
    "    # The initial conditions\n",
    "    z0 = np.array([500., 0., 0., 0., 0., 0.])\n",
    "    return scipy.integrate.odeint(g, z0, t, args=(x,))\n",
    "\n",
    "# The times at which we need to make predictions\n",
    "T = np.linspace(0, 180, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the reaction rates `k` have been transformed to `x` inside the solver. We follow the same formulation as shown in the class. Suppose we the following Gaussian prior for the reaction rates:\n",
    "$$\n",
    "p(\\mathbf{x}) = \\mathcal{N}(\\mathbf{x}|\\mathbf{0}, \\gamma^2 \\mathbf{I}),\n",
    "$$\n",
    "\n",
    "and the measurement process is defined by the following likelihood model:\n",
    "$$\n",
    "p(y|\\mathbf{x}, \\sigma^2, \\gamma) = \\mathcal{N}(y|f(\\mathbf{x}), \\sigma^2),\n",
    "$$\n",
    "where, $f(\\cdot)$ denotes the dynamical system model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1 - Constant $\\gamma$ and $\\sigma$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Treat the parameters, $\\gamma$ and $\\sigma$ as constant. The posterior distribution we wish to sample from is given by:\n",
    "\n",
    "$$\n",
    "\\pi(\\mathbf{x}) = p(\\mathbf{x}|y,\\sigma^2) \\propto p(y|\\mathbf{x},\\sigma^2)p(\\mathbf{x}) = \\mathcal{N}\\left(y|f(\\mathbf{x}),\\sigma^2\\right)\\mathcal{N}(0,\\gamma^2 \\mathbf{I}) \\propto \\exp\\left\\{-\\frac{\\parallel y - f(\\mathbf{x}) \\parallel_2^2}{2\\sigma^2}-\\frac{\\parallel \\mathbf{x}\\parallel^2_2}{2\\gamma^2}\\right\\}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a function which returns the log of the unnormalized posterior of $\\pi(\\cdot)$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use ```pyMC``` to generate samples from this distribution. You can simply follow the procedure outlined in handout 23 to do this. First, setup a function ```make_model``` in which you define all the probabilistic quantities of this problem. The ```make_model``` function, for this problem, is already defined below. It accepts the $\\gamma$ and $\\sigma$ parameters as input. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_model(gamma, sigma):\n",
    "    \"\"\"\n",
    "    PyMC model (wrapping all the data and variables into a single function)\n",
    "    \"\"\"\n",
    "    #get the observed data as a flattened array \n",
    "    Yobs = Y.flatten()\n",
    "    \n",
    "    # Define Prior\n",
    "    invgamma2 = gamma ** -2   #precision (= inverse of the covariance)\n",
    "    x = pm.MvNormal('x', np.zeros(5), invgamma2*np.eye(5))    \n",
    "    \n",
    "    #define the mean of the likelihood \n",
    "    @pm.deterministic\n",
    "    def fm(x=x):\n",
    "        tmp = Z(x, T)\n",
    "        Ym = np.hstack([tmp[:, :2], tmp[:, 3:]])\n",
    "        return Ym.flatten()\n",
    "    \n",
    "    # Define Likelihood model\n",
    "    invsigma2 = sigma ** -2   #precision of the observed data \n",
    "    observation = pm.Normal(\"obs\", mu=fm, tau=invsigma2, value=Yobs, observed=True)\n",
    "    return locals()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your task is to do the following:\n",
    "\n",
    "1. Set up the MCMC sampler. \n",
    "\n",
    "2. Simulate the Markov Chain to generate samples of $x$. \n",
    "\n",
    "3. Perform model diagnostics i.e. figure out - a. What values of $\\gamma$ and $\\sigma$ to select, how many samples to burn and how many samples to thin from the Markov Chain.\n",
    "\n",
    "4. For the best model you find, do the triangle plots (pairwise joint posterior distributions and individual marginal distributions)\n",
    "\n",
    "5. Plot the data vs prediction plots with error bars.\n",
    "\n",
    "Feel free to re-use all/part of the code from the handouts. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Enter solution/code here. *\n",
    "<br><br><br><br><br><br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Problem 2 - Exponential prior over $\\sigma^2$ and $\\gamma^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now introduce prior specifications for $\\sigma^2$ and $\\gamma^2$. Let $p(\\sigma^2) = \\mathrm{Exp}(\\sigma^2 | \\alpha_1)$ and $p(\\gamma^2) = \\mathrm{Exp}(\\gamma^2 | \\alpha_2)$, where, $\\mathrm{Exp}$ denotes the exponential distribution. This formulation introduces 2 additional parameters to tune - $\\alpha_1$ and $\\alpha_2$. Repeat tasks 1-5 from problem 1 for this case. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_model(alpha1, alpha2):\n",
    "    \"\"\"\n",
    "    PyMC model (wrapping all the data and variables into a single function)\n",
    "    \"\"\"\n",
    "    #get the observed data as a flattened array \n",
    "    Yobs = Y.flatten()\n",
    "    \n",
    "    # Define Prior\n",
    "    gamma2 = pm.Exponential('$\\gamma^2$', alpha1)\n",
    "    sigma2 = pm.Exponential('$\\sigma^2$', alpha2)\n",
    "    invgamma2 = gamma2 ** -1\n",
    "    x = pm.MvNormal('x', np.zeros(6), invgamma2*np.eye(6))\n",
    "    \n",
    "    \n",
    "    #define the mean of the likelihood \n",
    "    @pm.deterministic\n",
    "    def fm(x=x):\n",
    "        tmp = Z(x, T)[1:]\n",
    "        Ym = np.hstack([tmp[:, :2], tmp[:, 3:]])\n",
    "        return Ym.flatten()\n",
    "    \n",
    "    # Define Likelihood model\n",
    "    invsigma2 = sigma2 ** -1  #precision of the observed data \n",
    "    observation = pm.Normal(\"obs\", mu=fm, tau=invsigma2, value=Yobs, observed=True)\n",
    "\n",
    "    return locals()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "*Enter solution/code here. *\n",
    "<br><br><br><br><br><br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3 - Jeffrey's prior over $\\sigma^2$ and $\\gamma^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat tasks 1-5 from the previous question with the non-informative Jeffrey's prior specification on the two scale parameters, $\\sigma^2$, and $\\gamma^2$:\n",
    "$$\n",
    "p(\\sigma^2) \\propto \\frac{1}{\\sigma^2},\n",
    "$$\n",
    "\n",
    "$$\n",
    "p(\\gamma^2) \\propto \\frac{1}{\\gamma^2}.\n",
    "$$\n",
    "\n",
    "Modify the model specification appropriately. Note that we require custom definitions of the Jeffreys' prior on $\\sigma^2$ and $\\gamma^2$, as given in the code block below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_model():\n",
    "    \"\"\"\n",
    "    PyMC model (wrapping all the data and variables into a single function)\n",
    "    \"\"\"\n",
    "    #get the observed data as a flattened array \n",
    "    Yobs = Y.flatten()\n",
    "    \n",
    "    # Define Prior\n",
    "    @pm.stochastic(observed=False)\n",
    "    def jefsigma2(value=10):\n",
    "        if value <= 0:\n",
    "            return -np.Inf\n",
    "        #return the log likelihood \n",
    "        return -2 * np.log(value)\n",
    "\n",
    "    @pm.stochastic(observed=False)\n",
    "    def jefgamma2(value=10):\n",
    "        if value <= 0:\n",
    "            return -np.Inf\n",
    "        #return the log likelihood \n",
    "        return -2 * np.log(value)\n",
    "    invgamma2 = jefgamma2 ** -1\n",
    "    x = pm.MvNormal('x', np.zeros(6), invgamma2*np.eye(6))\n",
    "    \n",
    "    \n",
    "    #define the mean of the likelihood \n",
    "    @pm.deterministic\n",
    "    def fm(x=x):\n",
    "        tmp = Z(x, T)\n",
    "        Ym = np.hstack([tmp[:, :2], tmp[:, 3:]])\n",
    "        return Ym.flatten()\n",
    "    \n",
    "    # Define Likelihood model\n",
    "    invsigma2 = jefsigma2 ** -1  #precision of the observed data \n",
    "    observation = pm.Normal(\"obs\", mu=fm, tau=invsigma2, value=Yobs, observed=True)\n",
    "\n",
    "    return locals()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Enter solution/code here. *\n",
    "<br><br><br><br><br><br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 4 - A model with a different noise variance for each observed species."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now consider the following likelihood:\n",
    "$$\n",
    "p(y_i|\\mathbf{x}) = \\mathcal{N}(y_i|f(\\mathbf{x}), \\sigma_{i}^{2}),\n",
    "$$\n",
    "where $i$ is an index for observed chemical species.  In other words, we construct the likelihood model such that there is a different noise parameter, $\\sigma_i$ associated with the measurements obtained for each different species. Each $\\sigma_{i}^{2}$ is specified with a Jeffreys' prior, i.e., $p(\\sigma_{i}^{2}) \\propto \\frac{1}{\\sigma_{i}^{2}}$.\n",
    "\n",
    "Repeat tasks 1-5 from the previous for this likelihood model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_model():\n",
    "    \"\"\"\n",
    "    PyMC model (wrapping all the data and variables into a single function)\n",
    "    \"\"\"\n",
    "    #get the observed data as a flattened array \n",
    "    Yobs = Y.flatten(order='F')\n",
    "    \n",
    "    # Define Prior\n",
    "    @pm.stochastic(observed=False)\n",
    "    def jefsigma2_1(value=10):\n",
    "        if value <= 0:\n",
    "            return -np.Inf\n",
    "        #return the log likelihood \n",
    "        return -2 * np.log(value)\n",
    "    \n",
    "    @pm.stochastic(observed=False)\n",
    "    def jefsigma2_2(value=10):\n",
    "        if value <= 0:\n",
    "            return -np.Inf\n",
    "        #return the log likelihood \n",
    "        return -2 * np.log(value)\n",
    "    \n",
    "    @pm.stochastic(observed=False)\n",
    "    def jefsigma2_3(value=10):\n",
    "        if value <= 0:\n",
    "            return -np.Inf\n",
    "        #return the log likelihood \n",
    "        return -2 * np.log(value)\n",
    "    \n",
    "    @pm.stochastic(observed=False)\n",
    "    def jefsigma2_4(value=10):\n",
    "        if value <= 0:\n",
    "            return -np.Inf\n",
    "        #return the log likelihood \n",
    "        return -2 * np.log(value)\n",
    "    \n",
    "    @pm.stochastic(observed=False)\n",
    "    def jefsigma2_5(value=10):\n",
    "        if value <= 0:\n",
    "            return -np.Inf\n",
    "        #return the log likelihood \n",
    "        return -2 * np.log(value)\n",
    "    \n",
    "\n",
    "    @pm.stochastic(observed=False)\n",
    "    def jefgamma2(value=10):\n",
    "        if value <= 0:\n",
    "            return -np.Inf\n",
    "        #return the log likelihood \n",
    "        return -2 * np.log(value)\n",
    "    invgamma2 = jefgamma2 ** -1\n",
    "    x = pm.MvNormal('x', np.zeros(6), invgamma2*np.eye(6))\n",
    "    \n",
    "    #define the noise parameter of the likelihood \n",
    "    @pm.deterministic \n",
    "    def jefsigma2():\n",
    "        out = np.zeros(30)\n",
    "        out[:6] = jefsigma2_1\n",
    "        out[6:12] = jefsigma2_2\n",
    "        out[12:18] = jefsigma2_3\n",
    "        out[18:24] = jefsigma2_4\n",
    "        out[24:30] = jefsigma2_5\n",
    "        return out\n",
    "    \n",
    "    #define the mean of the likelihood \n",
    "    @pm.deterministic\n",
    "    def fm(x=x):\n",
    "        tmp = Z(x, T)\n",
    "        Ym = np.hstack([tmp[:, :2], tmp[:, 3:]])\n",
    "        return Ym.flatten(order='F')\n",
    "    \n",
    "    # Define Likelihood model\n",
    "    invsigma2 = jefsigma2 ** -1  #precision of the observed data \n",
    "    observation = pm.Normal(\"obs\", mu=fm, tau=invsigma2, value=Yobs, observed=True)\n",
    "\n",
    "    return locals()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "*Enter solution/code here. *\n",
    "<br><br><br><br><br><br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 5 - A model with concentration dependent noise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Redefine the likelihood as follows: \n",
    "$$\n",
    "p(y_i|\\mathbf{x}, \\sigma) = \\mathcal{N}(y_i|f(\\mathbf{x}), (\\sigma_i f(\\mathbf{x}))^2),\n",
    "$$\n",
    "where $i$ is an index for observed chemical species. Repeat tasks 1-5 from the previous for this likelihood model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Enter solution/code here. *\n",
    "<br><br><br><br><br><br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Problem 6 - Model selection using SMC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use Sequential Monte Carlo (SMC) to determine the model evidence of the 5 different models defined in problems 1 to 5. Make appropriate changes to each of the ```make_model``` functions defined in the previous questions and setup the ```pysmc``` sampler to compute the evidence. Which is the best model that you find? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Enter solution/code here. *\n",
    "<br><br><br><br><br><br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
