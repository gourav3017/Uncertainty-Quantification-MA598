{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 3\n",
    "\n",
    "**Due: 02/15/2018** (Thursday 15th February at 11:59pm).\n",
    "\n",
    "## Instructions\n",
    "\n",
    "+ In any case, develop the code and generate the figures you need to solve the problems using this notebook.\n",
    "+ For the answers that require a mathematical proof or derivation you can either:\n",
    "    \n",
    "    - Type the answer using the built-in latex capabilities. In this case, simply export the notebook as a pdf and upload it on gradescope; or\n",
    "    - you can print the notebook (after you are done with all the code), write your answers by hand, scan, turn your response to a single pdf, and upload on gradescope. \n",
    "\n",
    "+ The total homework points are 100. Please note that the problems are not weighed equally.\n",
    "\n",
    "**Note**: Please match all the pages corresponding to each of the questions when you submit on gradescope. \n",
    "\n",
    "## Student details\n",
    "\n",
    "+ **First Name:**\n",
    "+ **Last Name:**\n",
    "+ **Email:**\n",
    "\n",
    "## Readings\n",
    "\n",
    "Before attempting the homework, it is probably a good idea to:\n",
    "+ Read chapter 3 of Bishop (Pattern recognition and machine learning);\n",
    "+ Review the slides of lectures 7, 8, & 9; and\n",
    "+ Review the corresponding lecture handouts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "sns.set_context('paper')\n",
    "sns.set_style('white')\n",
    "sns.set()\n",
    "import scipy.stats as st\n",
    "from sklearn.datasets import make_spd_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose you conduct some experiments and collect a dataset of $N$ pairs of input and target variables, $\\mathcal{D} = (x_{1:N}, y_{1:N})$, where $x_i \\in \\mathbb{R}$ and $y_i \\in \\mathbb{R}$, $\\forall i$. \n",
    "Assume a Gaussian likelihood with the mean being a generalized linear model with weights $\\mathbf{w}\\in\\mathbb{R}^m$ and basis functions $\\boldsymbol{\\phi}(x)\\in\\mathbb{R}^M$, and the noise variance being a constant $\\sigma^2$.\n",
    "On the weights, use an isotropic Gaussian prior, with precision parameter, $\\alpha$.\n",
    "\n",
    "1. Derive expressions for $\\mathbf{m}_{N}$ and $\\mathbf{S}_{N}$, the posterior mean and covariance of the model parameters respectively. Ask the question: What do I know about the weights given all the data I have seen? You will need Bayes rule for updating the weights and little bit of algebra. In particular, you will need a trick called \"completing the square.\"\n",
    "\n",
    "2. Use the results from part 1 to derive the posterior predictive distribution at an arbitrary test input $x^{*}$. Ask the question: What do I know about the $y^*$ at $x^*$ given all the data I have seen? You will need the sum rule of probability theory to connect this question to the likehood and the posterior you obtained in step 1.\n",
    "\n",
    "3. Suppose now you perform an additional experiment and receive a data-point, $\\mathcal{D}_{N+1}=(x_{N+1}, y_{N+1})$. Using the current posterior distribution over the parameters as the new prior, show that updating the model with the $(N+1)^{th}$ data-point results in the same posterior distribution shown above, with $N$ replaced by $N+1$.\n",
    "\n",
    "The required expressions for all  of the above cases are well-known in closed form. It is, however, useful to work through the algebra atleast once. Feel free to consult Bishop's book, but in the end present your own derivation from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Enter your model description/solution here. Delete that ``<br>`` line (it just makes some white space).*\n",
    "<br><br><br><br><br><br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Conjugate priors](https://en.wikipedia.org/wiki/Conjugate_prior) are an extremely useful tool in Bayesian inference.\n",
    "If the posterior distribution over the unknown parameters, $\\boldsymbol{\\theta}$, of a statistical model is in the same family  of distributions as the prior, the prior  is said to be conjugate to the chosen likelihood. We saw one such example in class where a Gaussian prior over the unknown weights of the linear regression model lead to a Gaussian posterior under the Gaussian likelihood model. We used a fixed value of $\\sigma^2$ in our analysis of the linear regression model in class.\n",
    "\n",
    "As before, consider a Gaussian likelihood with the mean being a generalized linear model with weights $\\mathbf{w}\\in\\mathbb{R}^m$ and basis functions $\\boldsymbol{\\phi}(x)\\in\\mathbb{R}^M$, and the noise variance being a constant $\\sigma^2$\n",
    "Let's treat the noise parameter also as an unknown. Let $\\beta$ be the inverse noise variance, i.e., $\\beta = \\frac{1}{\\sigma^2}$ $^{(1)}$. \n",
    "Show that the following prior over $w$ and $\\beta$:\n",
    "$$\n",
    "p(\\mathbf{w}, \\beta) = \\mathcal{N}(\\mathbf{w}|0, \\alpha^{-1}\\mathbf{I}) \\mathrm{Gamma}(\\beta| a_0, b_0),\n",
    "$$\n",
    "is conjugate.\n",
    "That is, show that the posterior over $\\mathbf{w}$ and $\\beta$ has the same form as the prior: \n",
    "$$p(\\mathbf{w}, \\beta|\\mathcal{D}_N, \\alpha) = \\mathcal{N}(w|\\mathbf{m}_N, \\mathbf{S}_N) \\mathrm{Gamma}(\\beta| a_N, b_N).$$\n",
    "In doing so, recover the expressions for $\\mathbf{m}_N$, $\\mathbf{S}_N$, $a_N$ and $b_N$. Discuss any interesting observation you make about the form of the posterior distribution parameters.\n",
    "\n",
    "The [Gamma distribution](https://en.wikipedia.org/wiki/Gamma_distribution) has probability density:\n",
    "$$\n",
    "\\mathrm{Gamma}(\\beta|a_0, b_0) = \\frac{b_0^{a_0}}{\\Gamma(a)}\\beta^{a_0-1}e^{-b_0\\beta}\n",
    "$$\n",
    "\n",
    "(1) - _You will frequently encounter in literature the use of the precision rather than the variance when using the normal distribution. Doing so often simplifies computation_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Enter your model description/solution here. Delete that ``<br>`` line (it just makes some white space).*\n",
    "<br><br><br><br><br><br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 3 - Some exercises on the multivariate normal "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Bayesian linear regression model discussed in class makes extensive usage of the multivariate Gaussian distribution. ```numpy``` and ```scipy``` offer nice implementations of the multivariate normal distribution for computing densities and generating samples. However, it is useful to go through the process of developing your method for doing these things atleast once. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the random variable $\\mathbf{X} \\sim \\mathcal{N}(\\mathbf{X}|\\mu, \\Sigma)$, where, $\\mathbf{X} \\in \\mathbb{R}^d$ and $\\mu$ and $\\Sigma$ are its mean vector and covariance matrix respectively. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Density of a multivariate Gaussian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The expression for  the density of the multivariate Gaussian distribution can be found [here](https://en.wikipedia.org/wiki/Multivariate_normal_distribution). \n",
    "\n",
    "Note that evaluating the density function of MVN (multivariate normal) requires evaluating the inverse of the covariance matrix, $\\Sigma$. Inverting a matrix is inefficient and numerically unstable and should be avoided as much as possible. \n",
    "\n",
    "Instead you can compute the density of the random variable $\\mathbf{X}$ at an arbitrary point $\\mathbf{x}$ as follows:\n",
    "\n",
    "1. Use [```scipy.linalg.cho_factor```](https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.linalg.cho_factor.html#scipy.linalg.cho_factor) to perform  the Cholesky decomposition of $\\Sigma$ i.e. find $\\mathbf{L}$ such that $\\Sigma = \\mathbf{L} \\mathbf{L}^T$.\n",
    "2. Solve, for $\\mathbf{z}$, the system of linear equations $\\mathbf{L} \\mathbf{L}^T \\mathbf{z} = \\mathbf{x} -\\mu$. You can use [```scipy.linalg.cho_solve```](https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.linalg.cho_solve.html).\n",
    "3. Put everything together to compute $p(\\mathbf{x}) = \\frac{1}{\\sqrt{(2\\pi)^d | \\Sigma|}}\\exp\\Big[ -\\frac{1}{2}(\\mathbf{x}-\\mu)^T \\mathbf{z} \\Big]$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example with an arbitrary mean and covariance in 2 dimensions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.linalg import cho_factor, cho_solve\n",
    "d =2\n",
    "mean = np.array([1, 2])[:, None]\n",
    "cov = np.array([[2, 1], [1, 4]])\n",
    "L = cho_factor(cov, lower=True)\n",
    "diagL = np.diag(L[0])\n",
    "detcov = np.prod(diagL) ** 2  #Sigma = LL^T ; Determinant of prod =  prod. of determinant.\n",
    "Z = 1./np.sqrt(((2*np.pi)**2)*detcov)  #normalizing constant \n",
    "\n",
    "#define a grid over x \n",
    "x1 = np.linspace(-5, 10, 50)\n",
    "X1, X2 = np.meshgrid(x1, x1)\n",
    "Xgrid = np.hstack([X1.flatten()[:, None], X2.flatten()[:, None]])[:, :, None]\n",
    "Pdfs = np.array([Z*np.exp(-0.5*np.dot((xp-mean).T, cho_solve(L, xp-mean))) for xp in Xgrid])  ## See note below\n",
    "\n",
    "## For those new to Python, the above line uses the concept of list comprehensions in Python. \n",
    "## See here: http://www.secnetix.de/olli/Python/list_comprehensions.hawk\n",
    "## This is extremely useful for looping over simple expressions. \n",
    "## See also the map function: http://book.pythontips.com/en/latest/map_filter.html\n",
    "\n",
    "#visualize the density\n",
    "plt.contourf(X1, X2, Pdfs.reshape((50, 50)), 100, cmap = 'magma')\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function ```mvnpdf``` which accepts an input $\\mathbf{x}$ of any arbitrary dimension, $d$, and also accepts a mean vector and covariance matrix and returns the density of the normal distribution with given mean and covariance at point $\\mathbf{x}$. Feel free to re-use any/all code from the example given above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## write code here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note: You can assume that the density is non-degenerate, i.e., the covariance matrix is positive definite.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test your implementation. Use ```numpy.random.randn``` and ```sklearn.datasets.make_spd_matrix```  to generate random mean vector and covariance matrix, $\\mu$ and $\\Sigma$ for a random variable in $2$ dimensions. Visualize the contours of the density function. Use ```scipy.stats.multivariate_normal``` to verify that you get the correct result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# write code here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling from a multivariate Gaussian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that a univariate random variable, $\\mathbf{q} \\sim \\mathcal{N}({\\mathbf{q}|\\mu, \\sigma^2})$, can be expressed as $\\mathbf{q} = \\mu + \\sigma \\mathbf{z}$, where, $\\mathbf{z} \\sim \\mathcal{N}({\\mathbf{z}|0, 1})$ is a standard normal random variable. This suggests an easy approach for sampling from a univariate distribution with arbitrary mean and variance - Sample from the standard normal distribution $\\mathcal{N}(0, 1)$, scale the result by standard deviation $\\sigma$ and then translate by $\\mu$.\n",
    "\n",
    "The approach to sampling from a multivariate Gaussian is analogous to the univariate case. Here are the steps:\n",
    "1. Compute  the Cholesky decomposition of the covariance matrix $\\Sigma$ i.e. find $\\mathbf{L}$ such that $\\Sigma = \\mathbf{L} \\mathbf{L}^T$.\n",
    "2. Sample a vector $\\mathbf{z}$ from the multivariate standard normal in the given dimensions, i.e., $\\mathcal{N}(\\mathbf{0}_{d}, ,\\mathbf{I}_{d\\times d})$.\n",
    "3. Scale and shift: $\\mathbf{x} = \\mu + \\mathbf{L}\\mathbf{z}$.\n",
    "\n",
    "The code below samples from the MVN defined in the previous section of this question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nsamples = 1000\n",
    "samples = np.array([mean+np.dot(np.tril(L[0]), np.random.randn(2, 1)) for i in xrange(nsamples)])[:, :, 0]\n",
    "x1 = samples[:,0]\n",
    "x2 = samples[:,1]\n",
    "\n",
    "#plot samples and compare to the pdf\n",
    "plt.contourf(X1, X2, Pdfs.reshape((50, 50)), 100, cmap = 'magma')\n",
    "plt.colorbar()\n",
    "plt.scatter(x1, x2, marker='x')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the generated samples look like they have been drawn from the MVN defined earlier. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function ```mvnsamples``` which accepts as input the mean vector and covariance matrix of a multivariate distribution of any arbitrary dimension, $d$, and returns $n$ samples from the distribution. $n$ is also to be passed as a parameter to the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# type code here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test your implementation. For the same mean and covariance generated earlier, draw $n$ samples and visualize it with a scatter plot. Make sure to compare the scatter plot with the density contours to verify your sampler is implemented correctly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# type code here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 4 - Linear regression on noisy dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the following dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data =  np.loadtxt('hw3_data1.txt')\n",
    "X = data[0, :]\n",
    "Y = data[1, :]\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(X, Y, 'ro', label = 'Data')\n",
    "plt.xlabel('$x$', fontsize=14)\n",
    "plt.ylabel('$y$', fontsize=14)\n",
    "plt.legend(loc='best', fontsize = 14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will try to fit the following linear regression model for this dataset: \n",
    "$$\n",
    "f(x;\\mathbf{w}) = w_0 + w_1 x,\n",
    "$$\n",
    "where, $w_0$ and $w_1$ are model  parameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian linear regression (Part 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the additive noise model:\n",
    "$$\n",
    "y = f(x;\\mathbf{w}) + \\epsilon = w_0 + w_1 x + \\epsilon,\n",
    "$$\n",
    "where, $\\epsilon \\sim \\mathcal{N}(\\epsilon|0, \\sigma^2)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the following isotropic prior on the weights:\n",
    "$$\n",
    "p(\\mathbf{w}) = \\mathcal{N}(\\mathbf{w}|0, \\alpha^{-1}\\mathbf{I}).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The density function of multivariate Gaussians can be found [here](https://en.wikipedia.org/wiki/Multivariate_normal_distribution). We will take a look at how to efficiently compute the density of multivariate Gaussians later in the course but for the time being let's use [scipy's implementation](https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.stats.multivariate_normal.html) of the same to visualize the prior. \n",
    "\n",
    "Generate a grid of $w_0$ and $w_1$ values and use scipy's ```multivariate_normal.pdf``` method to compute the prior probability density at each location of the grid. Note that the prior mean and covariance are shown in the expression above. Show the contour plot of the prior pdf. If you aren't already familiar, check out [this tutorial](https://jakevdp.github.io/PythonDataScienceHandbook/04.04-density-and-contour-plots.html) on matplotlib contour plots.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate some samples of $\\mathbf{w}$ from the prior and visualize the corresponding. You can use ```numpy.multivariate_normal```. An example using arbitrary mean and covariance  is shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = np.array([1, 2])\n",
    "cov = np.array([[2, 0], [0, 2]])\n",
    "w_sample = np.random.multivariate_normal(mean = mean, cov = cov, size = 1)\n",
    "w_0 = w_sample[0, 0]\n",
    "w_1 = w_sample[0, 1]\n",
    "x = np.linspace(-2, 2, 100)\n",
    "\n",
    "plt.plot(x, w_0 + w_1*x, label='$f(\\mathbf{x};\\mathbf{w}) = w_0 + w_1 x$')\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel('$f(x;\\mathbf{w})$')\n",
    "plt.legend(loc='best', fontsize=14)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: Please make sure all samples of $f$ are shown in the same plot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# nsamples = 5 (whatever number you want) \n",
    "#\n",
    "# Sample and visualize\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function that accepts the prior precision $\\alpha$ and the noise variance $\\sigma^2$ and returns the posterior mean and covariance of $w$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def postmeanvar(a, sigma2):\n",
    "    \"\"\"\n",
    "    write code here to return posterior mean and covariance of w.\n",
    "    \"\"\"\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the posterior distribution over $w$ using scipy's ```multivariate_normal.pdf``` function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Visualize the posterior\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How is the posterior different from the prior?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Enter your model description/solution here. Delete that ``<br>`` line (it just makes some white space).*\n",
    "<br><br><br><br><br><br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot some samples from the posterior distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "#  Code to plot some samples from posterior \n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the mean and variance of the posterior predictive distribution. Make sure to distinguish between \n",
    "measurement noise and epistemic uncertainty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Visualize posterior predictive distribution.\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is a good idea to set aside a part of your dataset for the purpose of testing the accuracy of your trained model. \n",
    "Consider the following test dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testdata = np.loadtxt('hw3_data1_test.txt')\n",
    "Xtest = testdata[0, :]\n",
    "Ytest = testdata[1, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make predictions on the test inputs, ```Xtest```, using the posterior predictive distribution under the Bayesian model. Compare it to the least squares predictions. Recall that the  least squares estimate of $\\mathbf{w}$ is given by:\n",
    "$$\n",
    "\\mathbf{w}_{\\mathrm{LS}} = (\\mathbf{\\Phi}^T \\mathbf{\\Phi})^{-1} \\mathbf{\\Phi}^T y_{1:N}. \n",
    "$$\n",
    "Use ```numpy.lstsq``` to obtain $\\mathbf{w}_{\\mathrm{LS}}$. The prediction at a new test location $x^*$ is given by $y^* = \\mathbf{w}_{\\mathrm{LS}, 0} + \\mathbf{w}_{\\mathrm{LS}, 1}x^*$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "#  Ypred_ls =  #least squares prediction.\n",
    "#  Ypred_bayes =  #bayesian model prediction. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Which model (Bayesian or least squares) offers better predictions? Why do you think that is?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Enter your model description/solution here. Delete that ``<br>`` line (it just makes some white space).*\n",
    "<br><br><br><br><br><br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In what situations (if any) would you expect simple least squares regression to perform better than the Bayesian regression?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Enter your model description/solution here. Delete that ``<br>`` line (it just makes some white space).*\n",
    "<br><br><br><br><br><br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evidence approximation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Picking the hyperparameters $\\alpha$ and $\\sigma^2$ is tricky. In theory, the fully approach to modeling the uncertainty in the hyperparameters is  simple - put  priors on them and make predictions on test data by marginalizing wrt to the hyperparameters and model weights. In practice, the resulting integrals are intractable. A popular and easy to implement approach to hyperparameter selection is [cross validation](https://en.wikipedia.org/wiki/Cross-validation_(statistics)). The idea is to choose a set of hyperparameter values, train the model at each value in the set and test it predictive accuracy. Finally, you select the values of the hyperparameters that offer the best predictive capacity.\n",
    "\n",
    "A more systematic approach is to maximize the model evidence. The evidence is the marginal likelihood  of the data conditional on the hyperparameters, i.e., $p(y|x, \\alpha, \\sigma^2)$. \n",
    "Under the Gaussian likelihood and isotropic Gaussian prior model, the log evidence is given by:\n",
    "$$\n",
    "log p(y|x, \\alpha, \\beta) = \\frac{M}{2} \\log \\alpha + \\frac{N}{2} \\log \\beta - E(\\mathbf{m}) -\\frac{1}{2} \\log \\mathrm{det}(A) - \\frac{N}{2} \\log 2\\pi,\n",
    "$$\n",
    "where, \n",
    "$\\beta$ is the inverse noise variance (or precision), \n",
    "\n",
    "$$A = \\alpha \\mathbf{I} + \\beta \\Phi^T \\Phi,$$ $$\\mathbf{m} = \\beta A^{-1} \\Phi^T y_{1:N},$$\n",
    "\n",
    "and $M$ is the number of model parameters, which in this case is 2. \n",
    "\n",
    "The term $E(\\mathbf{m})$ is a regularized misfit term given by:\n",
    "$$\n",
    "E(\\mathbf{m}) = \\frac{\\beta}{2} \\| y_{1:N} - \\Phi \\mathbf{m} \\|_{2}^{2} + \\frac{\\alpha}{2} \\| \\mathbf{m} \\|_{2}^{2}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up a function ```evidence``` that accepts the prior precision, $\\alpha$ and the inverse noise variance, $\\beta$, and returns the value of the evidence function. Feel free to parameterize your implementation of the ```evidence``` in whatever way you see fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evidence():\n",
    "    \"\"\"\n",
    "    Set this up.\n",
    "    \"\"\"\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use a suitable second order unconstrained optimization routine from ```scipy.optimize``` to minimize the **negative log evidence**. A popular method is the [BFGS algorithm.](https://docs.scipy.org/doc/scipy/reference/optimize.minimize-bfgs.html). Be sure to read the documentation carefully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Set up the optimization routine and minimize the negative log evidence. \n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the estimates of the hyperparameters obtained by maximizing the evidence to recompute the posterior mean and variance of the model parameters under the constant prior precision and likelihood variance model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "# compute posterior mean and variance. \n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Does this differ from your earlier estimate of the posterior mean and variance?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Enter your model description/solution here. Delete that ``<br>`` line (it just makes some white space).*\n",
    "<br><br><br><br><br><br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualize the following:** \n",
    "1.  The posterior density of $\\mathbf{w}$.\n",
    "2.  A few models sampled from the posterior. \n",
    "3.  The posterior predictive distribution  with noise variance and epistemic uncertainty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "#  Visualizations.\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, use the model you just trained to make predictions on the  test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Ypred_ev =\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How do the predictions compare to the previous versions?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Enter your model description/solution here. Delete that ``<br>`` line (it just makes some white space).*\n",
    "<br><br><br><br><br><br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian linear regression  (Part 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now look at a somewhat more complicated example. The following dataset was generated using a molecular dynamics simulation of a plastic material (thanks to [Professor Alejandro Strachan](https://engineering.purdue.edu/MSE/people/ptProfile?id=33239) for sharing the data!).\n",
    "In particular, we took a rectangular chunk of the material and we started applying tensile forces along one dimension.\n",
    "What you see in the data set below is the instantaneous measurements of *strain* (percent enlogation of the material in the pulling direction) vs the normal *stress* (force per square area in MPa = $10^6 \\text{N}/m^2$).\n",
    "This [video](https://youtu.be/K6vOkQ5F9r0) will help you understand how the dataset was generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.loadtxt('stress_strain.txt')\n",
    "epsilon = data[:, 0]\n",
    "sigma = data[:, 1]\n",
    "fig, ax = plt.subplots(figsize = (10, 6))\n",
    "ax.plot(epsilon, sigma, '.')\n",
    "ax.set_xlabel('Strain $\\epsilon$', fontsize = 14)\n",
    "ax.set_ylabel('Stress $\\sigma$', fontsize = 14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a noisy dataset.\n",
    "We would like to process it in order to extract what is known as the [stress-strain curve](https://en.wikipedia.org/wiki/Stress–strain_curve) of the material.\n",
    "The stress-strain curve characterizes the type of the material (the chemical bonds, the crystaline structure, any defects, etc.).\n",
    "It is a required input to the equations of [elasticity](https://en.wikipedia.org/wiki/Elasticity_(physics)) otherwise known as a *constitutive relation*.\n",
    "\n",
    "### Part A\n",
    "The very first part of the stress-strain curve is very close to being linear.\n",
    "It is called the *elastic regime*.\n",
    "In that region, say $\\epsilon < \\epsilon_l=0.04$, the relationship between stress and strain is:\n",
    "$$\n",
    "\\sigma(\\epsilon) = E\\epsilon.\n",
    "$$\n",
    "The constant $E$ is known as the *Young modulus* of the material.\n",
    "Use a generalized linear model and Bayesian linear regression to:\n",
    "+ Compute the posterior of $E$ given the data;\n",
    "+ Visualize your epistemic and aleatory uncertainty about the stress-strain curve in the elastic regime;\n",
    "+ Take five plaussible samples of the linear stress-strain curve and visualize them.\n",
    "\n",
    "In your answer, you should first clearly describe your model in text using the notation of the lectures and then code the solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# enter code here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part B\n",
    "Now, come up with a generalized linear model that can capture the non-linear part of the stress-strain relation.\n",
    "Remember, you can use any model you want as soon as:\n",
    "+ it is linear in the parameters to be estimated,\n",
    "+ it clearly has a well-defined elastic regime (see Part A).\n",
    "\n",
    "Use your model to:\n",
    "+ Derive, compute, and visualize a probabilistic estimate of the peak of the stress-strain curve (the so-called *yield stress*). This is not necessarily going to be Gaussian or even analytically available;\n",
    "+ Visualize your epistemic and aleatory uncertainty about the stress-strain curve.\n",
    "+ Take five plaussible samples of the linear stress-strain curve and visualize them.\n",
    "\n",
    "In your answer, you should first clearly describe your model in text using the notation of the lectures and then code the solution.\n",
    "\n",
    "*Hint: You can use the Heavide step function to turn on or off models for various ranges of $\\epsilon$. The idea is quite simple. Here is a model that has the right form in the elastic regime and an arbitrary form in the non-linear regime:*\n",
    "$$\n",
    "f(\\epsilon) = E\\epsilon \\left[(1 - H(\\epsilon - \\epsilon_l)\\right] + g(\\epsilon;\\mathbf{w}_g)H(\\epsilon - \\epsilon_l),\n",
    "$$\n",
    "where\n",
    "$$\n",
    "H(x) = \\begin{cases}\n",
    "0,\\;\\text{if}\\;x < 0\\\\\n",
    "1,\\;\\text{otherwise}.\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Enter your model description/solution here. Delete that ``<br>`` line (it just makes some white space).*\n",
    "<br><br><br><br><br><br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part C\n",
    "\n",
    "The model you constructed in part B may have a disctontinuity at $\\epsilon=\\epsilon_l$.\n",
    "How can you enforce continuity of $\\sigma(\\epsilon)$ and its first derivative at that point?\n",
    "Can you reparameterize the model of part B, so that this condition is automatically satisfied?\n",
    "If yes, then repeat the analysis of part B."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Enter your model description/solution here. Delete that ``<br>`` line (it just makes some white space).*\n",
    "<br><br><br><br><br><br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Enter your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-End-"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
